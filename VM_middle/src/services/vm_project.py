# src/services/vm_project.py
from __future__ import annotations

import os
import re
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Mapping
import shutil
import json

import asyncio

import httpx
from bson import ObjectId
from fastapi import HTTPException

from src.core.config import settings
from src.dao.vm_project import VmProjectDAO
from src.schemas.vm_project import (
    ProcessItemIn,
    ProcessPatchIn,
    ProjectFileOut,
    StockInfo,
    StockPatchIn,
    VmProjectCreateIn,
    VmProjectListResponse,
    VmProjectListItem,
    VmProjectDetailOut,
    StockItemOut,
    StockItemsResponse,
)
from src.utils.nc_splitter import (
    extract_tool_numbers_from_paths,  # ([saved_paths]) -> List[int|None]
    process_nc_text,  # (nc_text: str, output_dir: str, base_filename_with_ext: str) -> List[str]
)
from src.utils.stock import lookup_stock_code, is_known_stock_code
from src.utils.xml_parser import (
    extract_material_ref_from_project_xml,  # -> Optional[(gid, aid, eid)]
    extract_tool_refs_in_order,  # (project_xml, wpid) -> [{gid, aid, eid, tool_element_id, ...}]
    match_dt_file_refs,  # (parsed, gid=..., aid=..., eid=..., wpid=...)
    parse_cutting_tool_13399_xml,
    parse_dt_file_xml,
    parse_material_xml,
)
from src.services.vm_file import VmFileService
from src.utils.stock import STOCK_ITEMS

import logging

logger = logging.getLogger(__name__)

# ---------------- ë‚´ë¶€ ìœ í‹¸ (ì •ê·œì‹) ----------------
_COORD_RE = re.compile(r"\b([xyz])\s+coordinates_mm\s*:\s*([+-]?\d+(?:\.\d+)?)", re.I)
_MIN_RE = re.compile(r"\bmin_([xyz])\s*[:=]\s*([+-]?\d+(?:\.\d+)?)", re.I)
_MAX_RE = re.compile(r"\bmax_([xyz])\s*[:=]\s*([+-]?\d+(?:\.\d+)?)", re.I)
_LEN_RE = re.compile(r"\b([xyz])\s*length_mm\s*[:=]\s*([+-]?\d+(?:\.\d+)?)", re.I)

_STOCK_SIZE_6NUM_RE = re.compile(
    r"^\s*-?\d+(?:\.\d+)?\s*,\s*-?\d+(?:\.\d+)?\s*,\s*-?\d+(?:\.\d+)?\s*,\s*"
    r"-?\d+(?:\.\d+)?\s*,\s*-?\d+(?:\.\d+)?\s*,\s*-?\d+(?:\.\d+)?\s*$"
)


def _stock_from_material_xml(material_xml: str) -> StockInfo:
    parsed = parse_material_xml(material_xml)
    name_for_mapping = parsed.get("material_identifier") or parsed.get("display_name")
    code = lookup_stock_code(name_for_mapping)

    x_min = y_min = z_min = None
    x_max = y_max = z_max = None
    x_len = y_len = z_len = None
    coords = {"x": None, "y": None, "z": None}

    for raw in parsed.get("param_lines") or []:
        line = str(raw)
        m = _COORD_RE.search(line)
        if m:
            axis, val = m.group(1).lower(), float(m.group(2))
            coords[axis] = val
            continue
        m = _MIN_RE.search(line)
        if m:
            axis, val = m.group(1).lower(), float(m.group(2))
            if axis == "x":
                x_min = val
            elif axis == "y":
                y_min = val
            else:
                z_min = val
            continue
        m = _MAX_RE.search(line)
        if m:
            axis, val = m.group(1).lower(), float(m.group(2))
            if axis == "x":
                x_max = val
            elif axis == "y":
                y_max = val
            else:
                z_max = val
            continue
        m = _LEN_RE.search(line)
        if m:
            axis, val = m.group(1).lower(), float(m.group(2))
            if axis == "x":
                x_len = val
            elif axis == "y":
                y_len = val
            else:
                z_len = val
            continue

    # length* ê°€ ìˆìœ¼ë©´ (0, length)
    if x_len is not None:
        x_min, x_max = 0.0, x_len
    if y_len is not None:
        y_min, y_max = 0.0, y_len
    if z_len is not None:
        z_min, z_max = 0.0, z_len

    # coordinates_mmë§Œ ìˆìœ¼ë©´ ìŒìˆ˜â†’(v,0), ì–‘ìˆ˜â†’(0,v)
    for axis in ("x", "y", "z"):
        v = coords[axis]
        if v is None:
            continue
        if axis == "x" and x_min is None and x_max is None:
            x_min, x_max = (v, 0.0) if v < 0 else (0.0, v)
        if axis == "y" and y_min is None and y_max is None:
            y_min, y_max = (v, 0.0) if v < 0 else (0.0, v)
        if axis == "z" and z_min is None and z_max is None:
            z_min, z_max = (v, 0.0) if v < 0 else (0.0, v)

    # í•œìª½ë§Œ ìˆìœ¼ë©´ ë‹¤ë¥¸ìª½ 0
    if x_min is not None and x_max is None:
        x_max = 0.0
    if y_min is not None and y_max is None:
        y_max = 0.0
    if z_min is not None and z_max is None:
        z_max = 0.0
    if x_max is not None and x_min is None:
        x_min = 0.0
    if y_max is not None and y_min is None:
        y_min = 0.0
    if z_max is not None and z_min is None:
        z_min = 0.0

    if None in (x_min, y_min, z_min, x_max, y_max, z_max):
        return StockInfo(
            stock_type=code,
            stock_size="0,0,0,0,0,0",
            reason="material_xml íŒŒì‹±(ê¸¸ì´/ìµœì†Œ/ìµœëŒ€ í˜¼í•© ì§€ì›)",
        )
    stock_size = f"{x_min},{y_min},{z_min},{x_max},{y_max},{z_max}"
    return StockInfo(
        stock_type=code, stock_size=stock_size, reason="material_xml íŒŒì‹± ì„±ê³µ"
    )


class VmProjectService:
    def __init__(self, dao: VmProjectDAO, vm_file_svc: VmFileService):
        self.dao = dao
        self.vm_file_svc = vm_file_svc

    # ---------------- ISO HTTP ----------------
    async def _iso_get(
        self,
        path: str,
        *,
        path_params: Dict[str, str] | None = None,
        query: Dict[str, Any] | None = None,
    ) -> Any:
        base = str(settings.ISO_API_URL).rstrip("/")
        url = base + path.format(**(path_params or {}))
        async with httpx.AsyncClient(timeout=120.0) as client:
            r = await client.get(url, params=query or {})
            r.raise_for_status()
            return r.json()

    async def _fetch_project_xml(self, *, eid: str, gid: str, aid: str) -> str:
        data = await self._iso_get(
            settings.ISO_PATH_PROJECT_DETAIL,
            path_params={"element_id": eid},
            query={"global_asset_id": gid, "asset_id": aid},
        )
        return data.get("data") or ""

    async def _fetch_material_xml(
        self, *, eid: str, gid: str, aid: str
    ) -> Optional[str]:
        data = await self._iso_get(
            settings.ISO_PATH_ASSET_DETAIL,
            path_params={"element_id": eid},
            query={"global_asset_id": gid, "asset_id": aid, "type": "dt_material"},
        )
        return data.get("data") or None

    async def _fetch_tool_xml(self, *, eid: str, gid: str, aid: str) -> Optional[str]:
        data = await self._iso_get(
            settings.ISO_PATH_ASSET_DETAIL,
            path_params={"element_id": eid},
            query={
                "global_asset_id": gid,
                "asset_id": aid,
                "type": "dt_cutting_tool_13399",
            },
        )
        return data.get("data") or None

    async def _fetch_dt_file_xml(
        self, *, eid: str, gid: str, aid: str
    ) -> Optional[str]:
        data = await self._iso_get(
            settings.ISO_PATH_ASSET_DETAIL,
            path_params={"element_id": eid},
            query={"global_asset_id": gid, "asset_id": aid, "type": "dt_file"},
        )
        return data.get("data") or None

    async def _list_dt_file_element_ids(self, *, gid: str, aid: str) -> List[str]:
        data = await self._iso_get(
            settings.ISO_PATH_ASSET_LIST,
            query={"global_asset_id": gid, "asset_id": aid, "type": "dt_file"},
        )
        return data.get("data") or data.get("items") or []

    async def _download_nc_text_by_keys(self, *, gid: str, aid: str, eid: str) -> str:
        """
        GET {ISO_API_URL}{ISO_PATH_FILE_DOWNLOAD_BY_KEYS}
          ?global_asset_id=...&asset_id=...&element_id=...
        """
        base = str(settings.ISO_API_URL).rstrip("/")
        url = base + settings.ISO_PATH_FILE_DOWNLOAD_BY_KEYS
        async with httpx.AsyncClient(timeout=180.0) as client:
            r = await client.get(
                url,
                params={
                    "global_asset_id": gid,
                    "asset_id": aid,
                    "element_id": eid,
                },
            )
            r.raise_for_status()
            try:
                return r.text
            except Exception:
                return r.content.decode("utf-8", errors="ignore")

    # ---------------- Stock ê³„ì‚° ----------------
    async def compute_stock_auto(self, payload: VmProjectCreateIn) -> StockInfo:
        # payload: gid, aid, eid, wpid  (ìŠ¤í‚¤ë§ˆê°€ ê°œì •ë˜ì—ˆë‹¤ê³  ê°€ì •)
        proj_xml = await self._fetch_project_xml(
            eid=payload.eid, gid=payload.gid, aid=payload.aid
        )

        mat_ref = extract_material_ref_from_project_xml(
            proj_xml
        )  # -> (gid2, aid2, eid2) | None
        if mat_ref:
            gid2, aid2, eid2 = mat_ref
            gid2 = gid2 or payload.gid
            aid2 = aid2 or payload.aid
            mxml = await self._fetch_material_xml(eid=eid2, gid=gid2, aid=aid2)
            if mxml:
                return _stock_from_material_xml(mxml)

        return StockInfo(
            stock_type=None,
            stock_size=None,
            reason="material ì¶”ì • ì‹¤íŒ¨(its_workpieces/ref_dt_material ë‹¨ì„œ ì—†ìŒ)",
        )

    # ---------------- í”„ë¡œì íŠ¸ íŒŒì¼ ë¹Œë” ----------------
    def _build_project_file(
        self, stock: StockInfo, process: Optional[List[ProcessItemIn]] = None
    ) -> ProjectFileOut:
        procs = process or []
        return ProjectFileOut(
            stock_type=stock.stock_type,
            stock_size=stock.stock_size,
            process_count=len(procs),
            process=procs,
        )

    # ---------------- DB ì´ˆì•ˆ ìƒì„±/ì¡°íšŒ/íŒ¨ì¹˜ ----------------
    async def create_from_iso(
        self, payload: VmProjectCreateIn
    ) -> Tuple[ObjectId, StockInfo, ProjectFileOut]:
        stock = await self.compute_stock_auto(payload)
        pf = self._build_project_file(stock, process=[])
        _id = await self.dao.insert_draft_from_iso(
            source="iso",
            gid=payload.gid,
            aid=payload.aid,
            eid=payload.eid,
            wpid=payload.wpid,
            project_file_draft=pf.model_dump(),
        )
        return _id, stock, pf

    async def get_project_file(
        self, _id: ObjectId, *, source: str = "draft"
    ) -> ProjectFileOut:
        if source == "file":
            merged = await self._load_current_project_json(
                _id
            )  # íŒŒì¼ ìˆìœ¼ë©´ íŒŒì¼, ì—†ìœ¼ë©´ draft ë°˜í™˜
            return ProjectFileOut(**merged)
        # ê¸°ë³¸: draft
        doc = await self.dao.get(_id)
        pf = (doc or {}).get("project_file_draft") or {}
        return ProjectFileOut(**pf)

    async def patch_stock(self, _id: ObjectId, patch: StockPatchIn) -> ProjectFileOut:
        # --- 0) ìŠ¤í†¡ ì½”ë“œ ì‚¬ì „ ê²€ì¦: ë¯¸ì •ì˜ ì½”ë“œë©´ ì°¨ë‹¨ ---
        if patch.stock_type is not None and not is_known_stock_code(patch.stock_type):
            raise HTTPException(
                status_code=400,
                detail={
                    "message": f"unknown stock_type: {patch.stock_type}",
                    "hint": "ì‚¬ì „ì— ì •ì˜ëœ ìŠ¤í†¡ ì½”ë“œë§Œ í—ˆìš©ë©ë‹ˆë‹¤. /api/v1/vm-project/stocks ë¡œ ëª©ë¡ì„ í™•ì¸í•˜ì„¸ìš”.",
                },
            )

        doc = await self.dao.get(_id)
        pf = (doc or {}).get("project_file_draft") or {}

        # --- DB ì´ˆì•ˆ: stock í•„ë“œë§Œ êµì²´ ---
        changed: dict[str, object] = {}
        if patch.stock_type is not None:
            pf["stock_type"] = patch.stock_type
            changed["stock_type"] = patch.stock_type
        if patch.stock_size is not None:
            pf["stock_size"] = patch.stock_size
            changed["stock_size"] = patch.stock_size

        await self.dao.update_project_file_draft(_id, pf)

        # --- íŒŒì¼: stock í•„ë“œë§Œ ë¨¸ì§€ ---
        try:
            res = await self._write_project_json_merged(_id, changed)
            merged = res["merged"]
        except HTTPException:
            # íŒŒì¼ì´ ì•„ì§ ì—†ì„ ìˆ˜ ìˆìŒ(ì´ˆê¸° ë‹¨ê³„) â†’ ì´ ê²½ìš° íŒŒì¼ ìƒì„± ìŠ¤í‚µí•˜ê³  draftë§Œ ìœ ì§€
            merged = pf

        # --- ìœ íš¨ì„± ì¬ê²€ì¦ì€ ë¨¸ì§€ëœ ìµœì¢… JSON ê¸°ì¤€ ---
        pf_model = ProjectFileOut(**merged)
        validation_errors = self._validate_project_file(pf_model)
        await self.dao.set_validation_result(
            _id,
            is_valid=(len(validation_errors) == 0),
            errors=validation_errors,
            next_status_if_valid="ready",
            next_status_if_invalid="needs-fix",
        )
        return pf_model

    async def patch_process(
        self, _id: ObjectId, patch: ProcessPatchIn
    ) -> ProjectFileOut:
        doc = await self.dao.get(_id)
        pf = (doc or {}).get("project_file_draft") or {}

        # --- DB ì´ˆì•ˆ: process í•„ë“œë§Œ êµì²´ ---
        new_list = [p.model_dump() for p in patch.process]
        pf["process"] = new_list
        pf["process_count"] = len(new_list)
        await self.dao.update_project_file_draft(_id, pf)

        # --- íŒŒì¼: process í•„ë“œë§Œ ë¨¸ì§€ ---
        try:
            res = await self._write_project_json_merged(
                _id,
                {"process": new_list, "process_count": len(new_list)},
            )
            merged = res["merged"]
        except HTTPException:
            merged = pf

        # --- ìœ íš¨ì„± ì¬ê²€ì¦(ë¨¸ì§€ëœ ìµœì¢… JSON ê¸°ì¤€) ---
        pf_model = ProjectFileOut(**merged)
        validation_errors = self._validate_project_file(pf_model)
        await self.dao.set_validation_result(
            _id,
            is_valid=(len(validation_errors) == 0),
            errors=validation_errors,
            next_status_if_valid="ready",
            next_status_if_invalid="needs-fix",
        )
        return pf_model

    # ---------------- ë¼ìš°í„°ìš© ë¯¸ë¦¬ë³´ê¸°(ë¹ˆ process) ----------------
    async def preview_from_iso(
        self, payload: VmProjectCreateIn
    ) -> Tuple[StockInfo, ProjectFileOut]:
        stock = await self.compute_stock_auto(payload)
        pf = self._build_project_file(stock, process=[])
        return stock, pf

    # ---------------- í’€ íŒŒì´í”„ë¼ì¸ ë¯¸ë¦¬ë³´ê¸° (NC í¬í•¨) ----------------
    async def preview_from_iso_with_nc(
        self, payload: VmProjectCreateIn
    ) -> Tuple[StockInfo, ProjectFileOut, Dict]:
        """
        - stock ê³„ì‚°
        - (main ë˜ëŠ” í•˜ìœ„ workplan)ì—ì„œ WS ìˆœì„œëŒ€ë¡œ tool refs ì¶”ì¶œ
        - dt_file ì„ íƒ(í”„ë¡œì íŠ¸/ì›Œí¬í”Œëœ í‚¤ ë§¤ì¹­) â†’ í‚¤ ê¸°ë°˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        - NC ë¶„í•  ì €ì¥ (tmp/<projname>/ncdata/ì›ë³¸ëª…_1/ì›ë³¸ëª…_1.í™•ì¥ì ...)
        - WS ìˆ˜ == ë¶„í•  ìˆ˜ ê²€ì¦
        - ê° WSì˜ tool XML íŒŒì‹± â†’ tool_data êµ¬ì„±
        - process ì±„ì›Œ ProjectFileOut ë°˜í™˜
        - ì¶”ê°€: ncdata.zip ìƒì„±, project.prj ìƒì„±
        """
        debug: Dict[str, Any] = {}

        # 1) í”„ë¡œì íŠ¸ XML
        proj_xml = await self._fetch_project_xml(
            eid=payload.eid, gid=payload.gid, aid=payload.aid
        )

        # 2) ì›Œí¬í”Œëœ í™•ì¸/WS ì¶”ì¶œ
        ws_refs = extract_tool_refs_in_order(
            proj_xml, wpid=payload.wpid
        )  # [{gid, aid, eid, tool_element_id, ...}]
        if payload.wpid:
            wp_id = re.escape(payload.wpid)
            main_hit = re.search(
                rf"<main_workplan>.*?<its_id>\s*{wp_id}\s*</its_id>",
                proj_xml,
                re.DOTALL | re.IGNORECASE,
            )
            child_hit = re.search(
                rf"<main_workplan>.*?<its_elements[^>]*xsi:type=\"workplan\"[^>]*>.*?<its_id>\s*{wp_id}\s*</its_id>",
                proj_xml,
                re.DOTALL | re.IGNORECASE,
            )
            if not (main_hit or child_hit):
                raise HTTPException(
                    status_code=404,
                    detail=f"workplan its_id='{payload.wpid}' not found under main_workplan",
                )
        debug["ws_count"] = len(ws_refs)

        # 3) ì†Œì¬
        stock = await self.compute_stock_auto(payload)

        # 4) (aid, eid) í›„ë³´ ìˆ˜ì§‘ (gid ê¸°ì¤€)
        pairs = await self._list_dtfile_pairs(gid=payload.gid)
        if not pairs:
            raise ValueError("dt_file í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # 5) í‚¤ ë§¤ì¹­ë˜ëŠ” dt_file ì„ ì •
        matched_eid: Optional[str] = None
        matched_aid: Optional[str] = None
        matched_info: Optional[Dict[str, Any]] = None

        for aid_try, eid_try in pairs:
            try:
                xml = await self._fetch_dt_file_xml(
                    eid=eid_try, gid=payload.gid, aid=aid_try
                )
            except Exception:
                continue
            if not xml:
                continue

            info = parse_dt_file_xml(xml) or {}
            if match_dt_file_refs(
                info,
                gid=payload.gid,
                aid=payload.aid,
                eid=payload.eid,
                wpid=payload.wpid,
            ):
                matched_eid = info.get("element_id") or eid_try
                matched_aid = aid_try
                matched_info = info
                break

        if not matched_eid or not matched_aid or not matched_info:
            raise ValueError(
                "í”„ë¡œì íŠ¸/ì›Œí¬í”Œëœ í‚¤ì™€ ì¼ì¹˜í•˜ëŠ” dt_fileì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            )

        # ğŸ‘‰ ì—¬ê¸° ì¶”ê°€: ë§¤ì¹­ëœ ì›ë³¸ dt_fileì˜ aid/eidë¥¼ debugì— ê¸°ë¡
        debug["dt_file"] = {"aid": matched_aid, "eid": matched_eid}

        # 6) íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        nc_text = await self._download_nc_text_by_keys(
            gid=payload.gid, aid=matched_aid, eid=matched_eid
        )
        if not nc_text:
            raise ValueError("dt_file ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

        # 7) ë¶„í•  ì €ì¥ (tmp/<proj>/ncdata/â€¦)
        proj_name = self._create_vm_project_name()
        work_dir = os.path.join("tmp", proj_name)
        ncdata_dir = os.path.join(work_dir, "ncdata")
        os.makedirs(ncdata_dir, exist_ok=True)

        base_filename_with_ext = matched_info.get("display_name") or "program.nc"
        saved_paths = process_nc_text(
            nc_text, ncdata_dir, base_filename_with_ext=base_filename_with_ext
        )
        debug["proj_name"] = proj_name
        debug["work_dir"] = work_dir
        debug["nc_saved_count"] = len(saved_paths)
        debug["nc_saved_sample"] = saved_paths[:2]

        # 8) WS ìˆ˜ == ë¶„í•  ìˆ˜
        if len(saved_paths) != len(ws_refs):
            raise ValueError(
                f"ì›Œí‚¹ìŠ¤í… ìˆ˜({len(ws_refs)})ì™€ NC ë¶„í•  ìˆ˜({len(saved_paths)}) ë¶ˆì¼ì¹˜"
            )

        # 9) íˆ´ ë²ˆí˜¸ ë¹„êµ
        nc_tools = extract_tool_numbers_from_paths(saved_paths)
        ws_tools_num: List[Optional[int]] = []
        for w in ws_refs:
            t_eid = w.get("tool_element_id") or w.get("eid")
            num = None
            if isinstance(t_eid, str):
                m = re.search(r"T(\d+)$", t_eid.strip(), re.IGNORECASE)
                if m:
                    num = int(m.group(1))
            ws_tools_num.append(num)
        debug["nc_tools"] = nc_tools
        debug["ws_tools"] = ws_tools_num

        # NC íˆ´ë²ˆí˜¸ì™€ ì›Œí‚¹ìŠ¤í… íˆ´ë²ˆí˜¸ ì¼ì¹˜ ì—¬ë¶€ ê²€ì‚¬
        mismatch_errors = self._compare_tool_numbers(nc_tools, ws_tools_num)
        debug["tool_numbers_ok"] = len(mismatch_errors) == 0
        debug["tool_number_mismatches"] = mismatch_errors

        # 10) process (ìƒëŒ€ê²½ë¡œ ê·œì¹™ ë°˜ì˜)
        process_items: List[ProcessItemIn] = []
        for idx, w in enumerate(ws_refs):
            tnum = nc_tools[idx] if nc_tools[idx] is not None else ws_tools_num[idx]

            eff = cr = teeth = None
            gid_t = w.get("gid") or payload.gid
            aid_t = w.get("aid") or payload.aid
            eid_t = w.get("eid") or w.get("tool_element_id")
            if eid_t and gid_t and aid_t:
                tool_xml = await self._fetch_tool_xml(eid=eid_t, gid=gid_t, aid=aid_t)
                if tool_xml:
                    parsed = parse_cutting_tool_13399_xml(tool_xml) or {}
                    vals = parsed.get("values") or {}
                    eff = vals.get("effective_cutting_diameter")
                    cr = vals.get("corner_radius")
                    teeth = vals.get("number_of_teeth")

            half_minus_cr = (
                (eff / 2 - cr)
                if (isinstance(eff, (int, float)) and isinstance(cr, (int, float)))
                else None
            )

            def _fmt6(x: Optional[float]) -> str:
                return "{:.6f}".format(x) if isinstance(x, (int, float)) else "null"

            tool_data = ",".join(
                [
                    str(tnum) if tnum is not None else "null",
                    _fmt6(eff),
                    _fmt6(cr),
                    _fmt6(half_minus_cr),
                    _fmt6(cr),
                    "null",
                    "null",
                    "null",
                    _fmt6(teeth),
                ]
            )

            # ê²½ë¡œ ê·œì¹™
            abs_path = saved_paths[idx]
            rel_path = os.path.relpath(abs_path, start=work_dir)
            rel_path_win = rel_path.replace("/", "\\")
            stem = os.path.splitext(os.path.basename(rel_path))[0]
            out_dir_win = os.path.join("result", stem).replace("/", "\\")

            process_items.append(
                ProcessItemIn(
                    file_path=rel_path_win,  # "ncdata\\...\\...nc"
                    output_dir_path=out_dir_win,  # "result\\<stem>"
                    tool_data=tool_data,
                )
            )

        project_file = ProjectFileOut(
            stock_type=stock.stock_type,
            stock_size=stock.stock_size,
            process_count=len(process_items),
            process=process_items,
        )

        # 11) ncdata.zip
        zip_out = os.path.join(work_dir, "ncdata")
        archive_path = shutil.make_archive(
            zip_out, "zip", root_dir=work_dir, base_dir="ncdata"
        )
        debug["ncdata_zip"] = archive_path

        # 12) project.prj
        iso_proj_eid = payload.eid or "project"
        prj_path = self._write_project_prj(work_dir, iso_proj_eid, project_file)
        debug["project_prj"] = prj_path

        return stock, project_file, debug

    # ---------------- ê¸°íƒ€ ----------------
    def _create_vm_project_name(self) -> str:
        now = datetime.now()
        ampm = "pp" if now.hour >= 12 else "ap"
        hour12 = now.hour % 12 or 12
        return f"{now.year}-{now.month:02d}-{now.day:02d}_{ampm}_{hour12}_{now.minute:02d}_{now.second:02d}"

    async def _build_process_from_workplan(
        self,
        proj_xml: str,
        wpid: Optional[str],
        *,
        default_gid: Optional[str] = None,
        default_aid: Optional[str] = None,
    ) -> List[ProcessItemIn]:
        """
        ì„ íƒëœ ì›Œí¬í”Œëœì˜ ì›Œí‚¹ìŠ¤í… ìˆœì„œëŒ€ë¡œ tool_dataë§Œ ì±„ì›Œ ProcessItemIn ë¦¬ìŠ¤íŠ¸ ìƒì„±.
        - file_path/output_dir_pathëŠ” None (NC ë¶„í•  ì „ ë¯¸ë¦¬ë³´ê¸° ìš©ë„)
        - wsì— gid/aid/eidê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ default_*ë¥¼ ì‚¬ìš©(ì—†ìœ¼ë©´ íˆ´ XML ì¡°íšŒ ìŠ¤í‚µ)
        """

        def _fmt6(x: Optional[float]) -> str:
            return "{:.6f}".format(x) if isinstance(x, (int, float)) else "null"

        ws_list = extract_tool_refs_in_order(proj_xml, wpid)
        out: List[ProcessItemIn] = []

        for ws in ws_list:
            gid_t = ws.get("gid") or default_gid
            aid_t = ws.get("aid") or default_aid
            eid_t = ws.get("eid") or ws.get("tool_element_id")

            eff = cr = teeth = None
            if gid_t and aid_t and eid_t:
                tool_xml = await self._fetch_tool_xml(eid=eid_t, gid=gid_t, aid=aid_t)
                if tool_xml:
                    parsed = parse_cutting_tool_13399_xml(tool_xml) or {}
                    vals = parsed.get("values") or {}
                    eff = vals.get("effective_cutting_diameter")
                    cr = vals.get("corner_radius")
                    teeth = vals.get("number_of_teeth")

            half_minus_cr = (
                (eff / 2 - cr)
                if (isinstance(eff, (int, float)) and isinstance(cr, (int, float)))
                else None
            )

            # Të²ˆí˜¸ëŠ” ì—¬ê¸°ì„  ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ null. (NC ë¶„í•  í›„ì—ëŠ” NCì—ì„œ ì¶”ì¶œí•œ ë²ˆí˜¸ë¥¼ ì‚¬ìš©)
            tool_data = ",".join(
                [
                    "null",  # tool_no
                    _fmt6(eff),
                    _fmt6(cr),
                    _fmt6(half_minus_cr),
                    _fmt6(cr),
                    "null",
                    "null",
                    "null",
                    _fmt6(teeth),
                ]
            )

            out.append(
                ProcessItemIn(
                    file_path=None,
                    output_dir_path=None,
                    tool_data=tool_data,
                )
            )

        return out

    async def _list_dtfile_pairs(self, *, gid: str) -> List[tuple[str, str]]:
        """
        GET /api/v3/assets?global_asset_id=...&type=dt_file
        -> [(aid, eid), ...] ë°˜í™˜
        """
        data = await self._iso_get(
            settings.ISO_PATH_ASSET_LIST,  # ë³´í†µ "/api/v3/assets"
            query={"global_asset_id": gid, "type": "dt_file"},
        )
        assets = data.get("assets") or data.get("data") or data.get("items") or []
        pairs: List[tuple[str, str]] = []
        for a in assets:
            if not isinstance(a, dict):
                continue
            if (a.get("type") or "").strip() != "dt_file":
                continue
            aid = (a.get("asset_id") or "").strip()
            eid = (a.get("element_id") or "").strip()
            if aid and eid:
                pairs.append((aid, eid))
        return pairs

    def _unique_basename(self, dirpath: str, basename_no_ext: str, ext: str) -> str:
        """
        dirpath ì•ˆì—ì„œ basename_no_ext.ext ì™€ ì¶©ëŒë‚˜ë©´ -2, -3 ... suffix ë¶™ì—¬ ìœ ë‹ˆí¬ íŒŒì¼ëª… ë°˜í™˜
        ë°˜í™˜: ì ˆëŒ€ê²½ë¡œ (dirpath/basename.ext)
        """
        candidate = os.path.join(dirpath, f"{basename_no_ext}{ext}")
        if not os.path.exists(candidate):
            return candidate
        i = 2
        while True:
            cand = os.path.join(dirpath, f"{basename_no_ext}-{i}{ext}")
            if not os.path.exists(cand):
                return cand
            i += 1

    def _write_project_prj(
        self, work_dir: str, base_name: str, project_file: ProjectFileOut
    ) -> str:
        """
        work_dir ì•ˆì— <base_name>.prj (ì¶©ëŒ ì‹œ -N)ë¡œ ì €ì¥.
        project_file(model) ë‚´ìš©ì„ JSONìœ¼ë¡œ ë¤í”„.
        """
        prj_path = self._unique_basename(work_dir, base_name, ".prj")
        data = project_file.model_dump()
        with open(prj_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return prj_path

    async def create_full_from_iso(self, payload: VmProjectCreateIn) -> dict:
        """
        1) stock/project_file ê³„ì‚° + nc ë¶„í• (zip), project.prj ìƒì„±
        2) vm_project ë¬¸ì„œ ìƒì„±(ì´ˆê¸° ìƒíƒœ)
        3) GridFS ì—…ë¡œë“œ + vm_file 2ê±´ ìƒì„±
        4) vm_project.latest_files í¬ì¸í„° ê°±ì‹  + ìƒíƒœ 'ready'
        """
        # 1) íŒŒì¼ ìƒì„± ë‹¨ê³„
        stock, project_file, debug = await self.preview_from_iso_with_nc(payload)
        zip_path = debug["ncdata_zip"]  # tmp/<proj>/ncdata.zip
        prj_path = debug["project_prj"]  # tmp/<proj>/<eid>[ -N].prj
        proj_name = debug.get("proj_name")
        dtfile_meta = debug.get("dt_file") or {}

        # 2) vm_project ìƒì„± (ì´ˆê¸°)
        vm_project_id = await self.dao.insert_initial_from_iso(
            source="iso",
            gid=payload.gid,
            aid=payload.aid,
            eid=payload.eid,
            wpid=payload.wpid,
            project_file_draft=project_file.model_dump(),
            proj_name=proj_name,
        )

        # 3) GridFS ì—…ë¡œë“œ + vm_file ìƒì„±
        # 3-1) ncdata.zip (íŒŒì¼ëª…ì€ ë¬´ì¡°ê±´ 'ncdata.zip')
        zip_vm_file_id = await self.vm_file_svc.create_from_path(
            vm_project_id=vm_project_id,
            kind="nc-split-zip",
            file_path=zip_path,
            original_name="ncdata.zip",  # <- ê³ ì •
            content_type="application/zip",
            meta={
                "source": "iso",
                "gid": payload.gid,
                "aid": payload.aid,
                "eid": payload.eid,
                "wpid": payload.wpid,
                "dt_file": {  # ì–´ë–¤ ì›ë³¸ dt_fileì—ì„œ ë¶„í• í–ˆëŠ”ì§€ ì¶”ì 
                    "aid": dtfile_meta.get("aid"),
                    "eid": dtfile_meta.get("eid"),
                },
            },
        )

        # 3-2) project.prj (JSON ë‚´ìš©, íŒŒì¼ëª…ì€ ì‹¤ì œ ìƒì„±ëœ ì´ë¦„ ê¸°ë¡)
        prj_vm_file_id = await self.vm_file_svc.create_from_path(
            vm_project_id=vm_project_id,
            kind="vm-project-json",
            file_path=prj_path,
            original_name=os.path.basename(prj_path),
            content_type="application/octet-stream",
            meta={
                "source": "iso",
                "gid": payload.gid,
                "aid": payload.aid,
                "eid": payload.eid,
                "wpid": payload.wpid,
                "process_count": project_file.process_count,
            },
        )

        # 4) vm_project ìµœì‹  í¬ì¸í„°/ìƒíƒœ ê°±ì‹ 
        await self.dao.set_latest_files(
            vm_project_id,
            {
                "nc-split-zip": zip_vm_file_id,
                "vm-project-json": prj_vm_file_id,
            },
        )
        # 5) ìœ íš¨ì„± ê²€ì‚¬ â†’ ìƒíƒœ ë°˜ì˜
        validation_errors = self._validate_project_file(project_file)

        tool_mismatch_errors = debug.get("tool_number_mismatches") or []
        if tool_mismatch_errors:
            validation_errors.extend(tool_mismatch_errors)

        await self.dao.set_validation_result(
            vm_project_id,
            is_valid=(len(validation_errors) == 0),
            errors=validation_errors,
            next_status_if_valid="ready",
            next_status_if_invalid="needs-fix",
        )

        return {
            "vm_project_id": str(vm_project_id),
            "status": "ready" if not validation_errors else "needs-fix",
            "files": {
                "nc_split_zip_id": str(zip_vm_file_id),
                "project_json_id": str(prj_vm_file_id),
            },
            "validation": {
                "is_valid": len(validation_errors) == 0,
                "errors": validation_errors,
            },
            "debug": debug,
        }

    def _validate_project_file(self, pf: ProjectFileOut) -> list[str]:
        errors: list[str] = []

        # stock
        if pf.stock_type is None:
            errors.append("stock_type is empty")
        elif not is_known_stock_code(pf.stock_type):
            errors.append(f"stock_type {pf.stock_type} is not allowed")

        if not pf.stock_size or not _STOCK_SIZE_6NUM_RE.match(pf.stock_size):
            errors.append("stock_size must be 6 numbers separated by commas")

        # process/tool_data
        if not isinstance(pf.process, list) or len(pf.process) == 0:
            errors.append("process is empty")
        else:
            for i, p in enumerate(pf.process, start=1):
                if not p.file_path:
                    errors.append(f"process[{i}].file_path is empty")
                if not p.output_dir_path:
                    errors.append(f"process[{i}].output_dir_path is empty")
                if not p.tool_data or "null" in p.tool_data.lower():
                    errors.append(f"process[{i}].tool_data contains null or is empty")
                parts = [x.strip() for x in (p.tool_data or "").split(",")]
                if len(parts) != 9:
                    errors.append(
                        f"process[{i}].tool_data must have 9 comma-separated fields"
                    )

        return errors

    async def list_projects(
        self,
        *,
        status: str | None = None,
        gid: str | None = None,
        aid: str | None = None,
        q: str | None = None,
        page: int = 1,
        size: int = 20,
    ) -> VmProjectListResponse:
        docs, total = await self.dao.list_projects(
            status=status, gid=gid, aid=aid, q=q, page=page, size=size
        )
        items: list[VmProjectListItem] = []
        for d in docs:
            val = d.get("validation") or {}
            errors = val.get("errors") or []
            items.append(
                VmProjectListItem(
                    id=str(d.get("_id")),
                    status=d.get("status"),
                    proj_name=d.get("proj_name"),
                    gid=d.get("gid"),
                    aid=d.get("aid"),
                    eid=d.get("eid"),
                    wpid=d.get("wpid"),
                    created_at=d.get("created_at"),
                    updated_at=d.get("updated_at"),
                    validation_is_valid=val.get("is_valid"),
                    validation_error_count=(
                        len(errors) if isinstance(errors, list) else 0
                    ),
                )
            )
        has_more = (page * size) < total
        return VmProjectListResponse(
            total=total, page=page, size=size, has_more=has_more, items=items
        )

    async def get_detail(self, _id: ObjectId) -> VmProjectDetailOut:
        doc = await self.dao.get(_id)
        if not doc:
            raise HTTPException(status_code=404, detail="vm_project not found")

        # latest_files ì˜ ObjectId â†’ str ë³€í™˜
        lf_raw = doc.get("latest_files") or {}
        latest_files: dict[str, str] = {}
        for k, v in lf_raw.items():
            try:
                latest_files[k] = str(v)
            except Exception:
                # í˜¹ì‹œ ì´ë¯¸ stringì´ë©´ ê·¸ëŒ€ë¡œ
                latest_files[k] = v if isinstance(v, str) else ""

        val = doc.get("validation") or {}
        errors = val.get("errors") or []
        is_valid = val.get("is_valid")

        pf_dict = doc.get("project_file_draft") or {}
        project_file = ProjectFileOut(**pf_dict)

        # ğŸ‘‡ vm í•„ë“œ ì¶”ì¶œ
        vm_job_id = doc.get("vm_job_id")
        vm_last_polled_at = doc.get("vm_last_polled_at")
        vm_error_message = doc.get("vm_error_message")
        vm_raw_status = doc.get("vm_raw_status")

        return VmProjectDetailOut(
            id=str(doc.get("_id")),
            status=doc.get("status"),
            proj_name=doc.get("proj_name"),
            gid=doc.get("gid"),
            aid=doc.get("aid"),
            eid=doc.get("eid"),
            wpid=doc.get("wpid"),
            created_at=doc.get("created_at"),
            updated_at=doc.get("updated_at"),
            latest_files=latest_files,
            validation_is_valid=is_valid,
            validation_errors=errors if isinstance(errors, list) else [],
            project_file_draft=project_file,
            vm_job_id=str(vm_job_id) if vm_job_id is not None else None,
            vm_last_polled_at=vm_last_polled_at,
            vm_error_message=vm_error_message,
            vm_raw_status=vm_raw_status,
        )

    async def list_stock_items(self, q: str | None = None) -> StockItemsResponse:
        """
        utils/stock.py ì˜ STOCK_ITEMS ë¥¼ ê·¸ëŒ€ë¡œ ë³´ì—¬ì¤€ë‹¤.
        - qê°€ ìˆìœ¼ë©´ ê°„ë‹¨ í•„í„°(ì½”ë“œëŠ” ì •í™• ì¼ì¹˜, ì´ë¦„ì€ ë¶€ë¶„ ì¼ì¹˜/ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)ë§Œ ì ìš©
        """
        items = STOCK_ITEMS

        if q:
            q_str = str(q).strip()
            filtered = []
            # ì½”ë“œ(ì •ìˆ˜) ì •í™• ì¼ì¹˜ ìš°ì„ 
            try:
                q_code = int(q_str)
            except ValueError:
                q_code = None

            for it in items:
                if q_code is not None and it.get("code") == q_code:
                    filtered.append(it)
                    continue
                name = str(it.get("name", ""))
                if q_str.lower() in name.lower():
                    filtered.append(it)
            items = filtered

        # ì¼ê´€ëœ ì •ë ¬(ì´ë¦„ ì˜¤ë¦„ì°¨ìˆœ)
        items = sorted(items, key=lambda x: str(x.get("name", "")))

        return StockItemsResponse(
            items=[
                StockItemOut(code=int(it["code"]), name=str(it["name"])) for it in items
            ]
        )

    def _compare_tool_numbers(
        self,
        nc_tools: List[Optional[int]],
        ws_tools: List[Optional[int]],
    ) -> list[str]:
        """
        NCì—ì„œ ì¶”ì¶œí•œ Të²ˆí˜¸ vs ì›Œí¬ìŠ¤í…ì—ì„œ ìœ ì¶”í•œ Të²ˆí˜¸ ë¹„êµ.
        - ë‘˜ ë‹¤ ìˆ«ìì¸ ê²½ìš°ë§Œ ë¹„êµí•˜ë©°, ë‹¤ë¥´ë©´ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±.
        - ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ Noneì´ë©´(= ì•Œ ìˆ˜ ì—†ìŒ) ì—¬ê¸°ì„œëŠ” ë¶ˆì¼ì¹˜ë¡œ ë³´ì§€ ì•Šê³ ,
        null ì²˜ë¦¬ ì—¬ë¶€ëŠ” _validate_project_fileì—ì„œ ê±¸ëŸ¬ëƒ„.
        """
        errors: list[str] = []
        for i, (n, w) in enumerate(zip(nc_tools, ws_tools), start=1):
            if n is not None and w is not None and n != w:
                errors.append(f"process[{i}]: tool number mismatch (nc={n}, ws={w})")
        return errors

    async def request_vm(self, _id: ObjectId) -> dict:
        doc = await self.dao.get(_id)
        if not doc:
            raise HTTPException(404, "vm_project not found")

        status = (doc.get("status") or "").strip()
        if status != "ready":
            raise HTTPException(
                400,
                detail={
                    "message": f"status must be 'ready' to request VM (current: {status})",
                    "validation": doc.get("validation") or {},
                },
            )

        pf_dict = doc.get("project_file_draft") or {}
        project_file = ProjectFileOut(**pf_dict)

        # 1) VM ì‹œìŠ¤í…œì— job ìƒì„±
        job_id = await self._vm_create_job(_id, project_file)

        # 2) ìš°ë¦¬ DBì— job_id + status=running ê¸°ë¡
        await self.dao.set_vm_job_id(_id, job_id)
        await self.dao.set_status(_id, "running")

        return {
            "vm_project_id": str(_id),
            "vm_job_id": job_id,
            "status": "running",
        }

    async def _load_current_project_json(self, vm_project_id: ObjectId) -> dict:
        """
        latest_files['vm-project-json']ê°€ ìˆìœ¼ë©´ GridFSì—ì„œ ì½ì–´ JSON ë°˜í™˜.
        ì—†ìœ¼ë©´ DB draftë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©.
        """
        doc = await self.dao.get(vm_project_id)
        pf_dict = (doc or {}).get("project_file_draft") or {}
        latest = (doc or {}).get("latest_files") or {}
        vmf_id = latest.get("vm-project-json")
        if not vmf_id:
            # íŒŒì¼ ì•„ì§ ì—†ìœ¼ë©´ draftë¥¼ ë°˜í™˜(íŒŒì¼ ìƒì„±ì€ í˜¸ì¶œë¶€ì—ì„œ íŒë‹¨)
            return dict(pf_dict)

        vmf_doc = await self.vm_file_svc.dao.get(vmf_id)
        if not vmf_doc:
            return dict(pf_dict)

        grid_id = vmf_doc.get("gridfs_id")
        if not grid_id:
            return dict(pf_dict)

        data = await self.vm_file_svc.filestore.gfs_get_bytes(grid_id)
        try:
            return json.loads(data.decode("utf-8"))
        except Exception:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ draftë¡œ í´ë°±
            return dict(pf_dict)

    async def _write_project_json_merged(
        self,
        vm_project_id: ObjectId,
        patch: Mapping[
            str, object
        ],  # ì˜ˆ: {"stock_type": 45, "stock_size": "..."} ë˜ëŠ” {"process": [...], "process_count": 7}
    ) -> dict:
        """
        - í˜„ì¬ JSON ë¡œë“œ â†’ patch í‚¤ë§Œ ë®ì–´ì“°ê¸°(ë¶€ë¶„ ìˆ˜ì •)
        - ìƒˆ JSONì„ GridFSì— ì—…ë¡œë“œ
        - vm_file.gridfs_id í¬ì¸í„°ë§Œ êµì²´(êµ¬ íŒŒì¼ ì‚­ì œ)
        - ë°˜í™˜: {"vm_file_id", "old_gridfs_id", "new_gridfs_id", "merged"}
        """
        # í˜„ì¬ JSON í™•ë³´
        current = await self._load_current_project_json(vm_project_id)
        merged = dict(current)
        for k, v in patch.items():
            merged[k] = v

        # latest vm-project-json vm_file
        proj_doc = await self.dao.get(vm_project_id)
        latest = (proj_doc or {}).get("latest_files") or {}
        vmf_id = latest.get("vm-project-json")
        if not vmf_id:
            # ì•„ì§ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•´ë„ ë˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” "ì—†ë‹¤"ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì—ëŸ¬ ì²˜ë¦¬
            # í•„ìš” ì‹œ: self.vm_file_svc.create_from_path(...) ë¡œ ìƒˆë¡œ ë§Œë“¤ë„ë¡ ë¶„ê¸° ê°€ëŠ¥
            raise HTTPException(400, detail="vm-project-json not found")

        vmf_doc = await self.vm_file_svc.dao.get(vmf_id)
        if not vmf_doc:
            raise HTTPException(404, detail="vm_file not found")

        old_grid = vmf_doc.get("gridfs_id")
        original_name = vmf_doc.get("original_name") or "project.prj"
        content_type = vmf_doc.get("content_type") or "application/octet-stream"

        data_bytes = json.dumps(merged, ensure_ascii=False, indent=2).encode("utf-8")

        new_grid = await self.vm_file_svc.filestore.gfs_put_bytes(
            data_bytes,
            filename=original_name,
            content_type=content_type,
            metadata={"source": "patch"},
        )

        await self.vm_file_svc.dao.update_gridfs_pointer(vmf_id, new_grid)

        # êµ¬ íŒŒì¼ì€ ì •ì±…ì— ë”°ë¼ ì‚­ì œ(ë³´ê´€ ì›í•˜ë©´ ì£¼ì„)
        if old_grid:
            try:
                await self.vm_file_svc.filestore.gfs_delete(old_grid)
            except Exception:
                pass

        return {
            "vm_file_id": vmf_id,
            "old_gridfs_id": old_grid,
            "new_gridfs_id": new_grid,
            "merged": merged,
        }

    # ==========VM í˜¸ì¶œ ê´€ë ¨ ==========
    async def start_vm_job(self, vm_project_id: ObjectId) -> Dict[str, Any]:
        """
        1) statusê°€ readyì¸ì§€ í™•ì¸ (ì•„ë‹ˆë©´ 400)
        2) í”„ë¡œì íŠ¸ JSON, NC ZIPì„ GridFSì—ì„œ êº¼ë‚´ VM S3 ì—…ë¡œë“œ APIë¡œ ê°ê° ì—…ë¡œë“œ
           - query param: parent_path = proj_name (project_idëŠ” ë„£ì§€ ì•ŠìŒ)
           - ì‘ë‹µì—ì„œ S3 ê²½ë¡œ ë¬¸ìì—´ì„ ì¶”ì¶œ(ì—†ìœ¼ë©´ ì—ëŸ¬)
        3) í† í° ë°œê¸‰ (username/password from settings)
        4) VM ìƒì„± API í˜¸ì¶œ (machine_name=eid, upload_file_link1/2 = ê²½ë¡œ)
        5) ì„±ê³µ ì‹œ vm_job_id / state / vm_last_polled_at / status=running ì—…ë°ì´íŠ¸
           ì‹¤íŒ¨ ì‹œ vm_error_messageë§Œ ê¸°ë¡í•˜ê³  ì—ëŸ¬ ë¦¬í„´
        """
        # 0) í”„ë¡œì íŠ¸ ë¬¸ì„œ ì¡°íšŒ ë° ìƒíƒœ í™•ì¸
        doc = await self.dao.get(vm_project_id)
        if not doc:
            raise HTTPException(404, "vm_project not found")

        status = (doc.get("status") or "").strip()
        existing_job_id = doc.get("vm_job_id")

        # ì´ë¯¸ running ìƒíƒœì¸ í”„ë¡œì íŠ¸ëŠ” ì¬ì‹œì‘ ë¶ˆê°€
        if status == "running":
            raise HTTPException(
                status_code=400,
                detail={
                    "message": "ì´ë¯¸ VM ì‘ì—…ì´ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.",
                    "status": status,
                    "vm_job_id": existing_job_id,
                },
            )

        # readyì¸ë°ë„ vm_job_idê°€ ë‚¨ì•„ ìˆìœ¼ë©´ ì¬ì‹œì‘ ë§‰ê¸° (ë°ì´í„° ì •í•©ì„± ë³´í˜¸)
        if status == "ready" and existing_job_id:
            raise HTTPException(
                status_code=400,
                detail={
                    "message": "ì´ë¯¸ VM ì‘ì—… IDê°€ í• ë‹¹ëœ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ì¬ì‹œì‘í•˜ë ¤ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.",
                    "status": status,
                    "vm_job_id": existing_job_id,
                },
            )

        if status != "ready":
            # needs-fix / completed / failed ë“±
            raise HTTPException(
                status_code=400,
                detail={
                    "message": "statusê°€ 'ready' ìƒíƒœì—ì„œë§Œ VM ì‘ì—…ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "status": status,
                },
            )

        proj_name = doc.get("proj_name")
        if not proj_name:
            raise HTTPException(400, "vm_project has no proj_name")

        latest_files = doc.get("latest_files") or {}
        nc_file_id = latest_files.get("nc-split-zip")
        prj_file_id = latest_files.get("vm-project-json")
        if not nc_file_id or not prj_file_id:
            raise HTTPException(400, "vm_project has no latest vm files")
        # 1) S3 ì—…ë¡œë“œ: project JSON / ncdata.zip
        project_s3_path = await self._vm_upload_latest_file(
            vm_file_id=prj_file_id,
            parent_path=proj_name,
        )
        nc_s3_path = await self._vm_upload_latest_file(
            vm_file_id=nc_file_id,
            parent_path=proj_name,
        )

        # 2) í† í° ë°œê¸‰
        token = await self._vm_issue_token()

        # 3) VM job ìƒì„± í˜¸ì¶œ
        try:
            vm_resp = await self._vm_create_job(
                token=token,
                machine_name=str(doc.get("eid") or ""),
                project_s3_path=project_s3_path,
                nc_s3_path=nc_s3_path,
            )
        except HTTPException as e:
            # ìƒì„± ì‹¤íŒ¨ ì‹œ: ì—ëŸ¬ ë©”ì‹œì§€ë§Œ ê¸°ë¡í•´ë‘ê³  ê·¸ëŒ€ë¡œ ì „íŒŒ
            await self.dao.set_vm_error(
                vm_project_id,
                message=str(e.detail),
                vm_raw_status=None,
            )
            raise

        # 4) ì‘ë‹µì—ì„œ job_id / state ì¶”ì¶œ
        vm_job_id = self._extract_job_id(vm_resp)
        if not vm_job_id:
            # idê°€ ì—†ìœ¼ë©´ ìš°ë¦¬ ìª½ì—” ì•„ë¬´ê²ƒë„ ê¸°ë¡í•˜ì§€ ì•Šê³  ì—ëŸ¬
            await self.dao.set_vm_error(
                vm_project_id,
                message="VM create-job did not return id",
                vm_raw_status=vm_resp,
            )
            raise HTTPException(502, "VM create-job did not return id")

        vm_state = self._extract_state(vm_resp)

        # 5) DBì— job ì‹œì‘ ì •ë³´ ê¸°ë¡
        await self.dao.set_vm_job_started(
            vm_project_id,
            vm_job_id=str(vm_job_id),
            vm_state=vm_state,
        )

        return {
            "vm_project_id": str(vm_project_id),
            "status": "running",
            "vm_job_id": str(vm_job_id),
            "vm_state": vm_state,
        }

    # ---------- ë‚´ë¶€ ìœ í‹¸: VM íŒŒì¼ ì—…ë¡œë“œ ----------
    async def _vm_upload_latest_file(
        self,
        *,
        vm_file_id: ObjectId,
        parent_path: str,
    ) -> str:
        """
        GridFSì—ì„œ íŒŒì¼ë°”ì´íŠ¸ë¥¼ ì½ì–´ VM ì—…ë¡œë“œ APIë¡œ ì „ì†¡.
        - Query: parent_path=<proj_name>
        - Body: multipart/form-data, file í•„ë“œ
        - ì‘ë‹µì—ì„œ ì—…ë¡œë“œëœ íŒŒì¼ URL(ê²½ë¡œ) ë¬¸ìì—´ ì¶”ì¶œ(ì—†ìœ¼ë©´ 502)
        """

        # 1) vm_file ë„íë¨¼íŠ¸ ì¡°íšŒ
        vmf_doc = await self.vm_file_svc.dao.get(vm_file_id)
        if not vmf_doc:
            raise HTTPException(404, detail="vm_file not found")

        gridfs_id = vmf_doc.get("gridfs_id")
        if not gridfs_id:
            raise HTTPException(400, detail="vm_file has no gridfs_id")

        # 2) GridFSì—ì„œ íŒŒì¼ ë‚´ìš© ë¡œë“œ
        content = await self.vm_file_svc.filestore.gfs_get_bytes(gridfs_id)
        filename = vmf_doc.get("original_name") or "file.bin"

        base = str(settings.VM_API_URL).rstrip("/")  # VM API ë² ì´ìŠ¤ URL
        url = base + str(settings.VM_S3_UPLOAD_DETAIL)

        logger.info(
            "VM /s3-upload call: url=%s, parent_path=%s, filename=%s, size=%d",
            url,
            parent_path,
            filename,
            len(content) if content is not None else -1,
        )

        # 3) ì—…ë¡œë“œ ìš”ì²­ (requests ë²„ì „ê³¼ ìµœëŒ€í•œ ìœ ì‚¬í•˜ê²Œ ì „ì†¡)
        try:
            async with httpx.AsyncClient(timeout=180.0, http2=False) as client:
                r = await client.post(
                    url,
                    params={"parent_path": parent_path},  # project_idëŠ” ì‚¬ìš© ì•ˆ í•¨
                    # ğŸ”½ requestsì˜ `files={"file": f}` ì™€ ìµœëŒ€í•œ ë¹„ìŠ·í•˜ê²Œ
                    files={"file": (filename, content)},
                )
        except httpx.ConnectError as e:
            # DNS ì‹¤íŒ¨, ì—°ê²° ì‹¤íŒ¨ ë“±
            logger.error("VM upload connect error: %s", e)
            raise HTTPException(
                status_code=502,
                detail=f"VM upload connection error: {e}",
            )
        except httpx.HTTPError as e:
            # ê¸°íƒ€ HTTP ë ˆë²¨ ì—ëŸ¬
            logger.error("VM upload HTTP error: %s", e)
            raise HTTPException(
                status_code=502,
                detail=f"VM upload HTTP error: {e}",
            )

        # 4) ìƒíƒœ ì½”ë“œ í™•ì¸
        try:
            r.raise_for_status()
        except httpx.HTTPStatusError as e:
            logger.error(
                "VM upload failed: status=%s, text=%s",
                e.response.status_code,
                e.response.text,
            )
            raise HTTPException(
                status_code=502,
                detail=f"VM upload failed: {e.response.text}",
            )

        # 5) JSON íŒŒì‹± + ê²½ë¡œ ì¶”ì¶œ
        data = self._safe_json(r)  # ì´ë¯¸ êµ¬í˜„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
        s3_path = self._extract_s3_path(data)  # file_url ë“±ì—ì„œ ì¶”ì¶œ

        if not s3_path:
            # "ë¦¬í„´ ê°’ì— ê²½ë¡œê°’ì´ ì œëŒ€ë¡œ ì˜¤ì§€ ì•Šì•˜ë‹¤ë©´ ì—ëŸ¬" ìš”êµ¬ì‚¬í•­
            logger.error("VM upload did not return path. response=%s", data)
            raise HTTPException(
                status_code=502,
                detail="VM upload did not return path",
            )

        logger.info("VM /s3-upload success: path=%s", s3_path)
        return s3_path

    async def _vm_issue_token(self) -> str:
        base = str(settings.VM_API_URL).rstrip("/")
        url = base + str(settings.VM_LOGIN_TOKEN)
        token_body = {
            "username": settings.VM_USERNAME,
            "password": settings.VM_PASSWORD,
        }
        async with httpx.AsyncClient(timeout=60.0) as client:
            r = await client.post(url, data=token_body)
            try:
                r.raise_for_status()
            except httpx.HTTPStatusError as e:
                raise HTTPException(502, detail=f"VM token error: {e.response.text}")

            data = self._safe_json(r)

            # âœ… ìŠ¤í™ì— ë§ì¶° ê¹”ë”í•˜ê²Œ
            access_token = data.get("access_token")
            if not access_token:
                raise HTTPException(
                    502, detail="VM token response missing access_token"
                )

            # token_typeë„ í•„ìš”í•˜ë©´ ê°™ì´ ì¨ë„ ë¨ (ê¸°ë³¸ bearer)
            token_type = (data.get("token_type") or "bearer").capitalize()
            # _vm_create_job ìª½ì—ì„œ: headers={"Authorization": f"{token_type} {access_token}"}

            return str(access_token)

    async def _vm_create_job(
        self,
        *,
        token: str,
        machine_name: str,
        project_s3_path: str,
        nc_s3_path: str,
    ) -> Dict[str, Any]:
        base = str(settings.VM_API_URL).rstrip("/")
        # ìƒì„± ì—”ë“œí¬ì¸íŠ¸ëŠ” configì— ì„ ì–¸ë¼ ìˆë‹¤ê³  ê°€ì •
        url = base + str(settings.VM_JOB_CREATE)

        body = {
            "machine_name": machine_name,
            "upload_file_link1": project_s3_path,
            "upload_file_link2": nc_s3_path,
        }
        headers = {"Authorization": f"Bearer {token}"}

        async with httpx.AsyncClient(timeout=120.0) as client:
            r = await client.post(url, json=body, headers=headers)
            try:
                r.raise_for_status()
            except httpx.HTTPStatusError as e:
                raise HTTPException(502, detail=f"VM create error: {e.response.text}")
            return self._safe_json(r)

    def _extract_state(self, data: Any) -> Optional[str]:
        if isinstance(data, dict):
            v = data.get("state")
            if v is not None:
                return str(v)
        return None

    @staticmethod
    def _safe_json(r: httpx.Response) -> Dict[str, Any]:
        try:
            return r.json()
        except Exception:
            return {"raw": r.text}

    @staticmethod
    def _extract_s3_path(data: Dict[str, Any]) -> Optional[str]:
        """
        /s3-upload ì‘ë‹µì—ì„œ ì—…ë¡œë“œëœ íŒŒì¼ì˜ URL/ê²½ë¡œë¥¼ ì¶”ì¶œí•œë‹¤.

        í˜„ì¬ ìŠ¤í™:
        {
          "file_url": "https://kitech-file.s3.ap-northeast-2.amazonaws.com/...."
        }
        """
        if not isinstance(data, dict):
            return None

        candidates = [
            data.get("file_url"),  # âœ… í˜„ì¬ ìŠ¤í™
            data.get("path"),
            data.get("s3_path"),
            data.get("url"),
            (
                (data.get("data") or {}).get("file_url")
                if isinstance(data.get("data"), dict)
                else None
            ),
            (
                (data.get("data") or {}).get("path")
                if isinstance(data.get("data"), dict)
                else None
            ),
        ]

        for c in candidates:
            if isinstance(c, str) and c.strip():
                return c.strip()
        return None

    @staticmethod
    def _extract_job_id(data: Dict[str, Any]) -> Optional[str]:
        """
        ìƒì„± ì‘ë‹µì—ì„œ job ì‹ë³„ìë¥¼ ê´€ìš©ì ìœ¼ë¡œ ì¶”ì¶œ:
        - data["id"] or data["_id"] or data["job_id"] or data["data"]["id"] ...
        """
        if not isinstance(data, dict):
            return None
        candidates = [
            data.get("id"),
            data.get("_id"),
            data.get("job_id"),
            (
                (data.get("data") or {}).get("id")
                if isinstance(data.get("data"), dict)
                else None
            ),
            (
                (data.get("result") or {}).get("id")
                if isinstance(data.get("result"), dict)
                else None
            ),
        ]
        for c in candidates:
            if c is None:
                continue
            return str(c)
        return None

    # =========== í’€ë§ê´€ë ¨ ===========
    async def poll_all_running_once(self) -> dict:
        """
        status='running' ì¸ vm_project ë“¤ì„ í•œ ë²ˆì”© í´ë§í•´ì„œ
        ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.
        """
        ids = await self.dao.list_running_ids()
        results: list[dict] = []

        if not ids:
            return {"polled_count": 0, "results": []}

        # ğŸ”½ ì—¬ê¸°ì—ì„œ í† í° í•œ ë²ˆë§Œ ë°œê¸‰
        try:
            token = await self._vm_issue_token()
        except HTTPException as e:
            logger.warning(
                "VM token issue failed while polling all running: %s", e.detail
            )
            # í† í° ëª» ë°›ì•˜ìœ¼ë©´ ì´ë²ˆ ë¼ìš´ë“œëŠ” ê·¸ëƒ¥ ìŠ¤í‚µ
            return {
                "polled_count": 0,
                "results": [],
                "error": f"token_issue_failed: {e.detail}",
            }

        for _id in ids:
            try:
                # ğŸ”½ í† í° ì¬ì‚¬ìš©
                res = await self.poll_vm_status(_id, token=token)
                results.append(res)
            except HTTPException as e:
                # ê°œë³„ í”„ë¡œì íŠ¸ í´ë§ ì‹¤íŒ¨ëŠ” ë¡œê·¸ë§Œ ì°ê³  ê³„ì† ì§„í–‰
                logger.warning("VM poll failed for project %s: %s", str(_id), e.detail)
            except Exception as e:
                logger.exception(
                    "Unexpected error while polling project %s: %s", str(_id), e
                )

        return {
            "polled_count": len(ids),
            "results": results,
        }

    async def vm_polling_loop(self, interval_sec: int = 300) -> None:
        """
        ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¬´í•œ ë£¨í”„ë¡œ ë™ì‘í•˜ë©´ì„œ
        ì¼ì • ê°„ê²©(interval_sec)ë§ˆë‹¤ poll_all_running_once() ë¥¼ í˜¸ì¶œí•œë‹¤.
        """
        logger.info("VM polling loop started (interval=%s sec)", interval_sec)
        while True:
            try:
                await self.poll_all_running_once()
            except Exception as e:
                # ì „ì²´ ë£¨í”„ ì—ëŸ¬ëŠ” ì¡ê³  ë¡œê·¸ë§Œ ë‚¨ê¸´ ë’¤ ë‹¤ìŒ ì£¼ê¸°ë¡œ ë„˜ì–´ê°
                logger.exception("Error in VM polling loop: %s", e)
            # ì§€ì •í•œ ì‹œê°„ë§Œí¼ ëŒ€ê¸°
            await asyncio.sleep(interval_sec)

    async def poll_vm_status(
        self,
        vm_project_id: ObjectId,
        token: str | None = None,  # â† í† í°ì„ ì„ íƒì ìœ¼ë¡œ ë°›ë„ë¡ ë³€ê²½
    ) -> dict:
        doc = await self.dao.get(vm_project_id)
        if not doc:
            raise HTTPException(404, "vm_project not found")

        job_id = doc.get("vm_job_id")
        if not job_id:
            raise HTTPException(400, "vm_job_id is empty")

        # ğŸ”½ ì—¬ê¸°ì„œ í† í°ì´ ì—†ìœ¼ë©´ í•œ ë²ˆë§Œ ë°œê¸‰
        if token is None:
            token = await self._vm_issue_token()

        base = str(settings.VM_API_URL).rstrip("/")
        url = base + str(settings.VM_GET_JOB_DETAIL_PATH).format(macsim_id=job_id)

        headers = {"Authorization": f"Bearer {token}"}

        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                r = await client.get(url, headers=headers)
                r.raise_for_status()
            except httpx.HTTPError as e:
                msg = getattr(e.response, "text", str(e))
                # í´ë§ ì‹¤íŒ¨ â†’ statusëŠ” ìœ ì§€, ì—ëŸ¬ ë©”ì‹œì§€ë§Œ ê¸°ë¡
                await self.dao.set_vm_poll_result(
                    vm_project_id,
                    status=doc.get("status", "running"),
                    vm_state=doc.get("vm_raw_status"),  # ì´ì „ state ìœ ì§€
                    vm_error_message=f"VM poll error: {msg}",
                )
                raise HTTPException(502, detail=f"VM poll error: {msg}")

        vm_resp = self._safe_json(r)
        vm_state = self._extract_state(vm_resp)

        # VM state â†’ ìš°ë¦¬ status ë§µí•‘
        if vm_state in ("WAIT", "RUNNING"):
            new_status = "running"
        elif vm_state in ("FINISH", "COMPLETED", "SUCCESS"):
            new_status = "completed"
        elif vm_state in ("ERROR", "FAILED", "CANCELED"):
            new_status = "failed"
        else:
            new_status = "running"  # ëª¨ë¥´ëŠ” ê°’ì´ë©´ ì¼ë‹¨ running ìœ ì§€

        await self.dao.set_vm_poll_result(
            vm_project_id,
            status=new_status,
            vm_state=vm_state,
            vm_error_message=None,
        )

        return {
            "vm_project_id": str(vm_project_id),
            "status": new_status,  # ë‚´ë¶€ ìƒíƒœ
            "vm_state": vm_state,  # VMì—ì„œ ì˜¨ state
            "vm_job_id": str(job_id),
        }
